{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7423166,"sourceType":"datasetVersion","datasetId":4319086}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -qqq -U peft trl datasets bitsandbytes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import (\n    LoraConfig,\n    PeftConfig,\n    PeftModel,\n    get_peft_model,\n    prepare_model_for_kbit_training,\n)\n\nfrom trl import SFTTrainer\n","metadata":{"execution":{"iopub.status.busy":"2024-01-27T18:39:16.888814Z","iopub.execute_input":"2024-01-27T18:39:16.889193Z","iopub.status.idle":"2024-01-27T18:39:24.344893Z","shell.execute_reply.started":"2024-01-27T18:39:16.889162Z","shell.execute_reply":"2024-01-27T18:39:24.343969Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"# The model that you want to train from the Hugging Face hub\nmodel_name = \"NousResearch/Llama-2-7b-chat-hf\"\n\n# The instruction dataset to use\ndataset_name = \"Akil15/evol_20k_filter\"\n\n# Fine-tuned model name\nnew_model = \"Akil15/finetune_llama_v_0.1\"\n","metadata":{"execution":{"iopub.status.busy":"2024-01-27T18:39:41.027328Z","iopub.execute_input":"2024-01-27T18:39:41.027717Z","iopub.status.idle":"2024-01-27T18:39:41.032664Z","shell.execute_reply.started":"2024-01-27T18:39:41.027687Z","shell.execute_reply":"2024-01-27T18:39:41.031599Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Bit-sandbytes configuration to downsize large model to small bit size model\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n)\n\n# # Check GPU compatibility with bfloat16\n# if compute_dtype == torch.float16 and use_4bit:\n#     major, _ = torch.cuda.get_device_capability()\n#     if major >= 8:\n#         print(\"=\" * 80)\n#         print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n#         print(\"=\" * 80)\n\n# Load base model\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code = True\n)\n\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\n# Load LLaMA tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n","metadata":{"execution":{"iopub.status.busy":"2024-01-27T18:39:43.786127Z","iopub.execute_input":"2024-01-27T18:39:43.786504Z","iopub.status.idle":"2024-01-27T18:40:44.461982Z","shell.execute_reply.started":"2024-01-27T18:39:43.786477Z","shell.execute_reply":"2024-01-27T18:40:44.460908Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d1ee4524f1d4faebcc92a8e46d90db2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a00587e42dee4cd08c936aabccb72e8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1840ae7ce5bf4cb4a1101545152932b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7c2597d9137409290a92cf889046262"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e21734c3978e481794b9af0317ecbc33"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1cac9f3f4e64dd891697940eb613d2d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/179 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"548f837dd6be4b7c806012e93aee1e21"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9297466612984adda62513a1641dda06"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24f9747e6155425f98659f7fd2400284"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e514bd902274be8ac7ae0eacdde5986"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0dff2df18594daeaf128fd794b02aa6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b58756b90f14aea9b286f56179d4fea"}},"metadata":{}}]},{"cell_type":"code","source":"model.gradient_checkpointing_enable()\nmodel = prepare_model_for_kbit_training(model)","metadata":{"execution":{"iopub.status.busy":"2024-01-27T18:41:34.553149Z","iopub.execute_input":"2024-01-27T18:41:34.553957Z","iopub.status.idle":"2024-01-27T18:41:34.581942Z","shell.execute_reply.started":"2024-01-27T18:41:34.553922Z","shell.execute_reply":"2024-01-27T18:41:34.581183Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"\n################################################################################\n# QLoRA parameters\n################################################################################\n\n# LoRA attention dimension\nlora_r = 64\n\n# Alpha parameter for LoRA scaling\nlora_alpha = 32\n\n# Dropout probability for LoRA layers\nlora_dropout = 0.05\n\n# Load LoRA configuration\npeft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_r,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\n\nmodel = get_peft_model(model, peft_config)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-27T18:41:43.795602Z","iopub.execute_input":"2024-01-27T18:41:43.796010Z","iopub.status.idle":"2024-01-27T18:41:44.309293Z","shell.execute_reply.started":"2024-01-27T18:41:43.795980Z","shell.execute_reply":"2024-01-27T18:41:44.308262Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"\n################################################################################\n# TrainingArguments parameters\n################################################################################\n\n# Output directory where the model predictions and checkpoints will be stored\noutput_dir = \"./results\"\n\n# Number of training epochs\nnum_train_epochs = 1\n\n# Enable fp16/bf16 training (set bf16 to True with an A100)\nfp16 = False\nbf16 = False\n\n# Batch size per GPU for training\nper_device_train_batch_size = 4\n\n# Batch size per GPU for evaluation\nper_device_eval_batch_size = 4\n\n# Number of update steps to accumulate the gradients for\ngradient_accumulation_steps = 4\n\n# Enable gradient checkpointing\ngradient_checkpointing = True\n\n# Maximum gradient normal (gradient clipping)\nmax_grad_norm = 0.3\n\n# Initial learning rate (AdamW optimizer)\nlearning_rate = 2e-4\n\n# Weight decay to apply to all layers except bias/LayerNorm weights\nweight_decay = 0.001\n\n# Optimizer to use\noptim = \"paged_adamw_32bit\"\n\n# Learning rate schedule\nlr_scheduler_type = \"cosine\"\n\n# Number of training steps (overrides num_train_epochs)\nmax_steps = -1\n\n# Ratio of steps for a linear warmup (from 0 to learning rate)\nwarmup_ratio = 0.03\n\n# Group sequences into batches with same length\n# Saves memory and speeds up training considerably\ngroup_by_length = True\n\n# Save checkpoint every X updates steps\nsave_steps = 0\n\n# Log every X updates steps\nlogging_steps = 25\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-27T18:44:02.877266Z","iopub.execute_input":"2024-01-27T18:44:02.877640Z","iopub.status.idle":"2024-01-27T18:44:02.884715Z","shell.execute_reply.started":"2024-01-27T18:44:02.877610Z","shell.execute_reply":"2024-01-27T18:44:02.883809Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"#  Set training parameters\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=num_train_epochs,\n    per_device_train_batch_size=per_device_train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    save_steps=save_steps,\n    logging_steps=logging_steps,\n    learning_rate=learning_rate,\n    weight_decay=weight_decay,\n    fp16=fp16,\n    bf16=bf16,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    group_by_length=group_by_length,\n    lr_scheduler_type=lr_scheduler_type,\n    report_to=\"tensorboard\"\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-27T18:44:05.374256Z","iopub.execute_input":"2024-01-27T18:44:05.375160Z","iopub.status.idle":"2024-01-27T18:44:05.381007Z","shell.execute_reply.started":"2024-01-27T18:44:05.375125Z","shell.execute_reply":"2024-01-27T18:44:05.380107Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"################################################################################\n# SFT parameters\n################################################################################\n\n# Maximum sequence length to use\nmax_seq_length = None\n\n# Pack multiple short examples in the same input sequence to increase efficiency\npacking = False\n\n# Load the entire model on the GPU 0\ndevice_map = {\"\": 0}","metadata":{"execution":{"iopub.status.busy":"2024-01-27T18:42:01.500816Z","iopub.execute_input":"2024-01-27T18:42:01.501567Z","iopub.status.idle":"2024-01-27T18:42:01.506191Z","shell.execute_reply.started":"2024-01-27T18:42:01.501534Z","shell.execute_reply":"2024-01-27T18:42:01.505193Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# List the files in the dataset directory\nimport os\n\ndataset_path = '//kaggle/input/instruct-set'\nfiles_in_dataset = os.listdir(dataset_path)\nprint(files_in_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-01-27T18:42:06.211500Z","iopub.execute_input":"2024-01-27T18:42:06.211967Z","iopub.status.idle":"2024-01-27T18:42:06.223042Z","shell.execute_reply.started":"2024-01-27T18:42:06.211933Z","shell.execute_reply":"2024-01-27T18:42:06.221933Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"['converted_alpaca_20k.csv']\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\n\n# Load the CSV file into a Pandas DataFrame\ncsv_file_path = os.path.join(dataset_path, 'converted_alpaca_20k.csv')\ndf = pd.read_csv(csv_file_path,low_memory=False)\n# dropping unecessary columns\ndf = df.iloc[:,:3]\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-27T18:42:09.994763Z","iopub.execute_input":"2024-01-27T18:42:09.995131Z","iopub.status.idle":"2024-01-27T18:42:10.456634Z","shell.execute_reply.started":"2024-01-27T18:42:09.995104Z","shell.execute_reply":"2024-01-27T18:42:10.455644Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"   input                                        instruction  \\\n0      0  Create an array of length 5 which contains all...   \n1      1  Formulate an equation to calculate the height ...   \n2      2  Write a replace method for a string class whic...   \n3      3  Create an array of length 15 containing number...   \n4      4  Write a function to find the number of distinc...   \n\n                                              output  \n0                             arr = [2, 4, 6, 8, 10]  \n1  Height of triangle = opposite side length * si...  \n2  def replace(self, replace_with):\\n    new_stri...  \n3  arr = [3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33...  \n4  def find_num_distinct_states(matrix):\\n    sta...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>input</th>\n      <th>instruction</th>\n      <th>output</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Create an array of length 5 which contains all...</td>\n      <td>arr = [2, 4, 6, 8, 10]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Formulate an equation to calculate the height ...</td>\n      <td>Height of triangle = opposite side length * si...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Write a replace method for a string class whic...</td>\n      <td>def replace(self, replace_with):\\n    new_stri...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Create an array of length 15 containing number...</td>\n      <td>arr = [3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>Write a function to find the number of distinc...</td>\n      <td>def find_num_distinct_states(matrix):\\n    sta...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from datasets import Dataset\n# Convert Pandas DataFrame to a datasets.Dataset\ncustom_dataset = Dataset.from_pandas(df)","metadata":{"execution":{"iopub.status.busy":"2024-01-27T18:42:12.650378Z","iopub.execute_input":"2024-01-27T18:42:12.651226Z","iopub.status.idle":"2024-01-27T18:42:12.724994Z","shell.execute_reply.started":"2024-01-27T18:42:12.651189Z","shell.execute_reply":"2024-01-27T18:42:12.724112Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Set supervised fine-tuning parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=custom_dataset,\n    peft_config=peft_config,\n    dataset_text_field=\"instruction\",\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing=packing\n)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-27T18:44:11.284129Z","iopub.execute_input":"2024-01-27T18:44:11.285019Z","iopub.status.idle":"2024-01-27T18:44:12.196077Z","shell.execute_reply.started":"2024-01-27T18:44:11.284984Z","shell.execute_reply":"2024-01-27T18:44:12.195327Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/20013 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9fe88f6c516f4b68b794450c4696207d"}},"metadata":{}}]},{"cell_type":"code","source":"model.config.use_cache = False\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-01-27T18:44:14.104316Z","iopub.execute_input":"2024-01-27T18:44:14.105085Z","iopub.status.idle":"2024-01-27T19:19:46.666479Z","shell.execute_reply.started":"2024-01-27T18:44:14.105056Z","shell.execute_reply":"2024-01-27T19:19:46.665143Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='200' max='1251' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 200/1251 34:25 < 3:02:41, 0.10 it/s, Epoch 0.16/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>2.747200</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>2.469800</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>1.649900</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.964300</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>1.571300</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.938500</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>1.499500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:323\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 323\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1537\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1535\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1537\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1854\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1851\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1853\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1854\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1856\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1857\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1859\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1860\u001b[0m ):\n\u001b[1;32m   1861\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1862\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2744\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2742\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   2743\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2746\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:1905\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1903\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1904\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1905\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"!huggingface-cli login","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\noutput_directory = '/kaggle/working/llama-2-7b-fine_tuned_v_0.1/'\nos.makedirs(output_directory, exist_ok=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-27T19:21:09.362345Z","iopub.execute_input":"2024-01-27T19:21:09.363022Z","iopub.status.idle":"2024-01-27T19:21:09.367952Z","shell.execute_reply.started":"2024-01-27T19:21:09.362988Z","shell.execute_reply":"2024-01-27T19:21:09.366785Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"import json\n\n\n# Convert TrainingArguments to dictionary\ntraining_args_dict = trainer.args.to_dict()\n\n# Save as JSON file\nwith open(\"/kaggle/working/llama-2-7b-fine_tuned_v_0.1/training_args.json\", \"w\") as json_file:\n    json.dump(training_args_dict, json_file)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-27T19:21:11.883498Z","iopub.execute_input":"2024-01-27T19:21:11.884371Z","iopub.status.idle":"2024-01-27T19:21:11.890318Z","shell.execute_reply.started":"2024-01-27T19:21:11.884337Z","shell.execute_reply":"2024-01-27T19:21:11.889429Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"training_args_dict","metadata":{"execution":{"iopub.status.busy":"2024-01-27T19:37:16.696572Z","iopub.execute_input":"2024-01-27T19:37:16.697546Z","iopub.status.idle":"2024-01-27T19:37:16.707197Z","shell.execute_reply.started":"2024-01-27T19:37:16.697511Z","shell.execute_reply":"2024-01-27T19:37:16.706224Z"},"trusted":true},"execution_count":39,"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"{'output_dir': './results',\n 'overwrite_output_dir': False,\n 'do_train': False,\n 'do_eval': False,\n 'do_predict': False,\n 'evaluation_strategy': 'no',\n 'prediction_loss_only': False,\n 'per_device_train_batch_size': 4,\n 'per_device_eval_batch_size': 8,\n 'per_gpu_train_batch_size': None,\n 'per_gpu_eval_batch_size': None,\n 'gradient_accumulation_steps': 4,\n 'eval_accumulation_steps': None,\n 'eval_delay': 0,\n 'learning_rate': 0.0002,\n 'weight_decay': 0.001,\n 'adam_beta1': 0.9,\n 'adam_beta2': 0.999,\n 'adam_epsilon': 1e-08,\n 'max_grad_norm': 0.3,\n 'num_train_epochs': 1,\n 'max_steps': -1,\n 'lr_scheduler_type': 'cosine',\n 'lr_scheduler_kwargs': {},\n 'warmup_ratio': 0.03,\n 'warmup_steps': 0,\n 'log_level': 'passive',\n 'log_level_replica': 'warning',\n 'log_on_each_node': True,\n 'logging_dir': './results/runs/Jan27_18-44-05_e9b373ccbedc',\n 'logging_strategy': 'steps',\n 'logging_first_step': False,\n 'logging_steps': 25,\n 'logging_nan_inf_filter': True,\n 'save_strategy': 'steps',\n 'save_steps': 0,\n 'save_total_limit': None,\n 'save_safetensors': True,\n 'save_on_each_node': False,\n 'save_only_model': False,\n 'no_cuda': False,\n 'use_cpu': False,\n 'use_mps_device': False,\n 'seed': 42,\n 'data_seed': None,\n 'jit_mode_eval': False,\n 'use_ipex': False,\n 'bf16': False,\n 'fp16': False,\n 'fp16_opt_level': 'O1',\n 'half_precision_backend': 'auto',\n 'bf16_full_eval': False,\n 'fp16_full_eval': False,\n 'tf32': None,\n 'local_rank': 0,\n 'ddp_backend': None,\n 'tpu_num_cores': None,\n 'tpu_metrics_debug': False,\n 'debug': [],\n 'dataloader_drop_last': False,\n 'eval_steps': None,\n 'dataloader_num_workers': 0,\n 'past_index': -1,\n 'run_name': './results',\n 'disable_tqdm': False,\n 'remove_unused_columns': True,\n 'label_names': None,\n 'load_best_model_at_end': False,\n 'metric_for_best_model': None,\n 'greater_is_better': None,\n 'ignore_data_skip': False,\n 'fsdp': [],\n 'fsdp_min_num_params': 0,\n 'fsdp_config': {'min_num_params': 0,\n  'xla': False,\n  'xla_fsdp_grad_ckpt': False},\n 'fsdp_transformer_layer_cls_to_wrap': None,\n 'deepspeed': None,\n 'label_smoothing_factor': 0.0,\n 'optim': 'paged_adamw_32bit',\n 'optim_args': None,\n 'adafactor': False,\n 'group_by_length': True,\n 'length_column_name': 'length',\n 'report_to': ['tensorboard'],\n 'ddp_find_unused_parameters': None,\n 'ddp_bucket_cap_mb': None,\n 'ddp_broadcast_buffers': None,\n 'dataloader_pin_memory': True,\n 'dataloader_persistent_workers': False,\n 'skip_memory_metrics': True,\n 'use_legacy_prediction_loop': False,\n 'push_to_hub': False,\n 'resume_from_checkpoint': None,\n 'hub_model_id': None,\n 'hub_strategy': 'every_save',\n 'hub_token': '<HUB_TOKEN>',\n 'hub_private_repo': False,\n 'hub_always_push': False,\n 'gradient_checkpointing': False,\n 'gradient_checkpointing_kwargs': None,\n 'include_inputs_for_metrics': False,\n 'fp16_backend': 'auto',\n 'push_to_hub_model_id': None,\n 'push_to_hub_organization': None,\n 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>',\n 'mp_parameters': '',\n 'auto_find_batch_size': False,\n 'full_determinism': False,\n 'torchdynamo': None,\n 'ray_scope': 'last',\n 'ddp_timeout': 1800,\n 'torch_compile': False,\n 'torch_compile_backend': None,\n 'torch_compile_mode': None,\n 'dispatch_batches': None,\n 'split_batches': False,\n 'include_tokens_per_second': False,\n 'include_num_input_tokens_seen': False,\n 'neftune_noise_alpha': None}"},"metadata":{}}]},{"cell_type":"code","source":"trainer.lr_scheduler.state_dict()","metadata":{"execution":{"iopub.status.busy":"2024-01-27T19:30:00.693061Z","iopub.execute_input":"2024-01-27T19:30:00.693961Z","iopub.status.idle":"2024-01-27T19:30:00.700466Z","shell.execute_reply.started":"2024-01-27T19:30:00.693930Z","shell.execute_reply":"2024-01-27T19:30:00.699469Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"{'base_lrs': [0.0002, 0.0002],\n 'last_epoch': 199,\n 'verbose': False,\n '_step_count': 200,\n '_get_lr_called_within_step': False,\n '_last_lr': [0.00019143163189119916, 0.00019143163189119916],\n 'lr_lambdas': [{}, {}]}"},"metadata":{}}]},{"cell_type":"code","source":"model.config","metadata":{"execution":{"iopub.status.busy":"2024-01-27T19:32:59.573628Z","iopub.execute_input":"2024-01-27T19:32:59.574536Z","iopub.status.idle":"2024-01-27T19:32:59.581822Z","shell.execute_reply.started":"2024-01-27T19:32:59.574502Z","shell.execute_reply":"2024-01-27T19:32:59.580835Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"LlamaConfig {\n  \"_name_or_path\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 11008,\n  \"max_position_embeddings\": 4096,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 32,\n  \"pad_token_id\": 0,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": null,\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 10000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"float16\",\n  \"transformers_version\": \"4.36.2\",\n  \"use_cache\": false,\n  \"vocab_size\": 32000\n}"},"metadata":{}}]},{"cell_type":"code","source":"import torch\n\n# Save optimizer and scheduler states\ntorch.save(trainer.optimizer.state_dict(), \"/kaggle/working/llama-2-7b-fine_tuned_v_0.1/optimizer_state.pth\")\ntorch.save(trainer.lr_scheduler.state_dict(), \"/kaggle/working/llama-2-7b-fine_tuned_v_0.1/scheduler_state.pth\")\nmodel.save_pretrained(\"/kaggle/working/llama-2-7b-fine_tuned_v_0.1/\")\n","metadata":{"execution":{"iopub.status.busy":"2024-01-27T19:21:19.413580Z","iopub.execute_input":"2024-01-27T19:21:19.413994Z","iopub.status.idle":"2024-01-27T19:21:20.826237Z","shell.execute_reply.started":"2024-01-27T19:21:19.413956Z","shell.execute_reply":"2024-01-27T19:21:20.825211Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nhf_key_val = user_secrets.get_secret(\"hf_key\")","metadata":{"execution":{"iopub.status.busy":"2024-01-27T19:21:54.608930Z","iopub.execute_input":"2024-01-27T19:21:54.609761Z","iopub.status.idle":"2024-01-27T19:21:54.844913Z","shell.execute_reply.started":"2024-01-27T19:21:54.609728Z","shell.execute_reply":"2024-01-27T19:21:54.844222Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import HfApi\napi = HfApi()","metadata":{"execution":{"iopub.status.busy":"2024-01-27T19:21:57.300399Z","iopub.execute_input":"2024-01-27T19:21:57.301195Z","iopub.status.idle":"2024-01-27T19:21:57.305361Z","shell.execute_reply.started":"2024-01-27T19:21:57.301164Z","shell.execute_reply":"2024-01-27T19:21:57.304419Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"api.upload_folder(folder_path = \"/kaggle/working/llama-2-7b-fine_tuned_v_0.1/\",\n                  path_in_repo = \".\",\n                  repo_id = \"Akil15/finetune_llama_v_0.1\",\n                  repo_type = \"model\",\n                  token =hf_key_val )","metadata":{"execution":{"iopub.status.busy":"2024-01-27T19:22:45.596408Z","iopub.execute_input":"2024-01-27T19:22:45.597306Z","iopub.status.idle":"2024-01-27T19:22:57.080011Z","shell.execute_reply.started":"2024-01-27T19:22:45.597274Z","shell.execute_reply":"2024-01-27T19:22:57.078903Z"},"trusted":true},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"scheduler_state.pth:   0%|          | 0.00/639 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9328cf6f0750480c823045dce43f5493"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/134M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec7591354bbd47aab2fd7e3d762f9c2a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16e088e0f1764e0b8b1e1c26b7e1557f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"optimizer_state.pth:   0%|          | 0.00/269M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c9d236ddf0d4f5a91c95b6b31c9a946"}},"metadata":{}},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Akil15/finetune_llama_v_0.1/commit/a6728072f0dab09b311bdac934ecb7b5f9ea3363', commit_message='Upload folder using huggingface_hub', commit_description='', oid='a6728072f0dab09b311bdac934ecb7b5f9ea3363', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"\ntokenizer.push_to_hub(\n    \"Akil15/finetune_llama_v_0.1\", use_auth_token=hf_key_val\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-27T19:27:41.802233Z","iopub.execute_input":"2024-01-27T19:27:41.802590Z","iopub.status.idle":"2024-01-27T19:27:43.321557Z","shell.execute_reply.started":"2024-01-27T19:27:41.802562Z","shell.execute_reply":"2024-01-27T19:27:43.320640Z"},"trusted":true},"execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf3cb53f4abb49da862681a7158f4ee9"}},"metadata":{}},{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Akil15/finetune_llama_v_0.1/commit/3ce60ef6fc7f2a036bc19f66c4f68f8d7164681e', commit_message='Upload tokenizer', commit_description='', oid='3ce60ef6fc7f2a036bc19f66c4f68f8d7164681e', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"\nmodel.push_to_hub(\n    \"Akil15/finetune_llama_v_0.1\", use_auth_token=hf_key_val\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-27T20:00:56.219319Z","iopub.execute_input":"2024-01-27T20:00:56.219629Z","iopub.status.idle":"2024-01-27T20:01:04.999797Z","shell.execute_reply.started":"2024-01-27T20:00:56.219601Z","shell.execute_reply":"2024-01-27T20:01:04.998929Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:821: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/134M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5528cfe653184007935a315af3af2290"}},"metadata":{}},{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Akil15/finetune_model_llama_v_0.1/commit/6a4fd97458a19b435dd4d77105fcee336fb292d0', commit_message='Upload model', commit_description='', oid='6a4fd97458a19b435dd4d77105fcee336fb292d0', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}